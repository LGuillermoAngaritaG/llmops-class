{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Summarizer: Model Setup and Monitoring\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Set up the YouTube summarizer model\n",
    "2. Implement monitoring using MLflow\n",
    "3. Track model performance metrics\n",
    "4. Monitor system resources and latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "import psutil\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Setup\n",
    "\n",
    "First, we'll set up our summarization model using the Hugging Face Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def initialize_model():\n",
    "    \"\"\"Initialize the summarization model\"\"\"\n",
    "    model_name = \"facebook/bart-large-cnn\"  # You can change this to other models\n",
    "    summarizer = pipeline(\"summarization\", model=model_name)\n",
    "    return summarizer\n",
    "\n",
    "# Initialize the model\n",
    "summarizer = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLflow Setup\n",
    "\n",
    "Set up MLflow to track experiments and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"youtube_summarizer_monitoring\")\n",
    "\n",
    "def log_metrics(metrics_dict):\n",
    "    \"\"\"Log metrics to MLflow\"\"\"\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_metrics(metrics_dict)\n",
    "        mlflow.log_param(\"model_name\", \"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Monitoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.metrics_history = []\n",
    "    \n",
    "    def measure_latency(self, func, *args, **kwargs):\n",
    "        \"\"\"Measure execution time of a function\"\"\"\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        return result, end_time - start_time\n",
    "    \n",
    "    def measure_resource_usage(self):\n",
    "        \"\"\"Measure CPU and memory usage\"\"\"\n",
    "        cpu_percent = psutil.cpu_percent()\n",
    "        memory_info = psutil.Process().memory_info()\n",
    "        return {\n",
    "            'cpu_percent': cpu_percent,\n",
    "            'memory_mb': memory_info.rss / 1024 / 1024\n",
    "        }\n",
    "    \n",
    "    def calculate_rouge_scores(self, prediction, reference):\n",
    "        \"\"\"Calculate ROUGE scores\"\"\"\n",
    "        scores = self.scorer.score(prediction, reference)\n",
    "        return {\n",
    "            'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "            'rouge2_f1': scores['rouge2'].fmeasure,\n",
    "            'rougeL_f1': scores['rougeL'].fmeasure\n",
    "        }\n",
    "    \n",
    "    def log_performance(self, latency, rouge_scores, resource_usage):\n",
    "        \"\"\"Log all performance metrics\"\"\"\n",
    "        metrics = {\n",
    "            'latency': latency,\n",
    "            **rouge_scores,\n",
    "            **resource_usage\n",
    "        }\n",
    "        self.metrics_history.append(metrics)\n",
    "        log_metrics(metrics)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_metrics_over_time(metrics_history):\n",
    "    \"\"\"Create interactive plots for metrics over time\"\"\"\n",
    "    df = pd.DataFrame(metrics_history)\n",
    "    \n",
    "    # Latency plot\n",
    "    fig_latency = px.line(df, y='latency', title='Inference Latency Over Time')\n",
    "    fig_latency.show()\n",
    "    \n",
    "    # Resource usage plot\n",
    "    fig_resources = go.Figure()\n",
    "    fig_resources.add_trace(go.Scatter(y=df['cpu_percent'], name='CPU %'))\n",
    "    fig_resources.add_trace(go.Scatter(y=df['memory_mb'], name='Memory (MB)'))\n",
    "    fig_resources.update_layout(title='Resource Usage Over Time')\n",
    "    fig_resources.show()\n",
    "    \n",
    "    # ROUGE scores plot\n",
    "    fig_rouge = go.Figure()\n",
    "    for metric in ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']:\n",
    "        fig_rouge.add_trace(go.Scatter(y=df[metric], name=metric))\n",
    "    fig_rouge.update_layout(title='ROUGE Scores Over Time')\n",
    "    fig_rouge.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the performance monitor\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "def process_video(video_id, reference_summary=None):\n",
    "    \"\"\"Process a video with monitoring\"\"\"\n",
    "    # Get transcript\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    text = ' '.join([t['text'] for t in transcript])\n",
    "    \n",
    "    # Generate summary with latency measurement\n",
    "    summary, latency = monitor.measure_latency(\n",
    "        lambda: summarizer(text, max_length=130, min_length=30)[0]['summary_text']\n",
    "    )\n",
    "    \n",
    "    # Measure resource usage\n",
    "    resource_usage = monitor.measure_resource_usage()\n",
    "    \n",
    "    # Calculate ROUGE scores if reference summary is provided\n",
    "    rouge_scores = monitor.calculate_rouge_scores(summary, reference_summary) if reference_summary else {}\n",
    "    \n",
    "    # Log all metrics\n",
    "    metrics = monitor.log_performance(latency, rouge_scores, resource_usage)\n",
    "    \n",
    "    return summary, metrics\n",
    "\n",
    "# Example usage\n",
    "# video_id = \"YOUR_VIDEO_ID\"\n",
    "# summary, metrics = process_video(video_id)\n",
    "# plot_metrics_over_time(monitor.metrics_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
