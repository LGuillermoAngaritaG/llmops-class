{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation for YouTube Summarizer with MLflow\n",
    "\n",
    "This notebook evaluates the YouTube summarizer model using MLflow to track metrics and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model_from_mlflow(run_id: str):\n",
    "    \"\"\"Load the YouTube summarizer model from MLflow\"\"\"\n",
    "    model_uri = f\"runs:/{run_id}/youtube_summarizer\"\n",
    "    return mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Replace with your run_id from the training notebook\n",
    "RUN_ID = \"your_run_id_here\"\n",
    "model = load_model_from_mlflow(RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_test_data() -> List[Dict[str, str]]:\n",
    "    \"\"\"Prepare test data with YouTube videos and reference summaries\"\"\"\n",
    "    # Replace with your actual test data\n",
    "    return [\n",
    "        {\n",
    "            \"video_url\": \"https://www.youtube.com/watch?v=example1\",\n",
    "            \"reference_summary\": \"Reference summary for video 1\"\n",
    "        },\n",
    "        {\n",
    "            \"video_url\": \"https://www.youtube.com/watch?v=example2\",\n",
    "            \"reference_summary\": \"Reference summary for video 2\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_metrics(predicted_summary: str, reference_summary: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate various evaluation metrics\"\"\"\n",
    "    # ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference_summary, predicted_summary)\n",
    "    \n",
    "    # BLEU score\n",
    "    reference = [reference_summary.split()]\n",
    "    candidate = predicted_summary.split()\n",
    "    bleu = sentence_bleu(reference, candidate)\n",
    "    \n",
    "    # Summary length metrics\n",
    "    pred_length = len(predicted_summary.split())\n",
    "    ref_length = len(reference_summary.split())\n",
    "    length_ratio = pred_length / ref_length if ref_length > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'rouge1_precision': rouge_scores['rouge1'].precision,\n",
    "        'rouge1_recall': rouge_scores['rouge1'].recall,\n",
    "        'rouge1_f1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2_f1': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL_f1': rouge_scores['rougeL'].fmeasure,\n",
    "        'bleu_score': bleu,\n",
    "        'summary_length_ratio': length_ratio,\n",
    "        'predicted_length': pred_length,\n",
    "        'reference_length': ref_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model_with_mlflow(model, test_data: List[Dict[str, str]]):\n",
    "    \"\"\"Evaluate model and log results to MLflow\"\"\"\n",
    "    mlflow.set_experiment(\"youtube-summarizer-evaluation\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"model_evaluation\") as run:\n",
    "        all_metrics = []\n",
    "        \n",
    "        # Log model parameters\n",
    "        model_params = model.get_config() if hasattr(model, 'get_config') else {}\n",
    "        mlflow.log_params(model_params)\n",
    "        \n",
    "        # Evaluate each test example\n",
    "        for i, example in enumerate(test_data):\n",
    "            # Generate summary\n",
    "            predicted_summary = model(example['video_url'])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = calculate_metrics(predicted_summary, example['reference_summary'])\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            # Log metrics for each example\n",
    "            for metric_name, value in metrics.items():\n",
    "                mlflow.log_metric(f\"example_{i}_{metric_name}\", value)\n",
    "            \n",
    "            # Log summaries as artifacts\n",
    "            example_dir = f\"example_{i}\"\n",
    "            os.makedirs(example_dir, exist_ok=True)\n",
    "            \n",
    "            with open(f\"{example_dir}/predicted_summary.txt\", \"w\") as f:\n",
    "                f.write(predicted_summary)\n",
    "            with open(f\"{example_dir}/reference_summary.txt\", \"w\") as f:\n",
    "                f.write(example['reference_summary'])\n",
    "            \n",
    "            mlflow.log_artifacts(example_dir)\n",
    "        \n",
    "        # Calculate and log average metrics\n",
    "        avg_metrics = {}\n",
    "        for metric in all_metrics[0].keys():\n",
    "            avg_value = np.mean([m[metric] for m in all_metrics])\n",
    "            avg_metrics[f\"avg_{metric}\"] = avg_value\n",
    "            mlflow.log_metric(f\"avg_{metric}\", avg_value)\n",
    "        \n",
    "        # Create and log visualizations\n",
    "        create_and_log_visualizations(all_metrics)\n",
    "        \n",
    "        return run.info.run_id, avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_and_log_visualizations(metrics_list: List[Dict[str, float]]):\n",
    "    \"\"\"Create and log visualizations to MLflow\"\"\"\n",
    "    # Convert metrics to DataFrame\n",
    "    df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # ROUGE scores comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    rouge_metrics = ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']\n",
    "    df[rouge_metrics].mean().plot(kind='bar')\n",
    "    plt.title('Average ROUGE Scores')\n",
    "    plt.ylabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rouge_scores.png')\n",
    "    mlflow.log_artifact('rouge_scores.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Summary length analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['reference_length'], df['predicted_length'])\n",
    "    plt.plot([0, max(df['reference_length'])], [0, max(df['reference_length'])], '--', color='red')\n",
    "    plt.xlabel('Reference Summary Length')\n",
    "    plt.ylabel('Predicted Summary Length')\n",
    "    plt.title('Summary Length Comparison')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('length_comparison.png')\n",
    "    mlflow.log_artifact('length_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Metrics distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics_to_plot = ['rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'bleu_score']\n",
    "    df[metrics_to_plot].boxplot()\n",
    "    plt.title('Distribution of Evaluation Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_distribution.png')\n",
    "    mlflow.log_artifact('metrics_distribution.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare test data\n",
    "test_data = prepare_test_data()\n",
    "\n",
    "# Run evaluation\n",
    "run_id, avg_metrics = evaluate_model_with_mlflow(model, test_data)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"==================\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nMLflow run ID: {run_id}\")\n",
    "print(\"View detailed results in the MLflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results in MLflow UI\n",
    "\n",
    "To view the detailed results and visualizations:\n",
    "1. Start the MLflow UI by running `mlflow ui` in your terminal\n",
    "2. Open http://localhost:5000 in your browser\n",
    "3. Navigate to the experiment \"youtube-summarizer-evaluation\"\n",
    "4. Click on the run ID printed above to see detailed metrics and artifacts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
