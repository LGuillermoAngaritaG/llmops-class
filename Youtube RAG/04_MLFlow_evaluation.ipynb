{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube RAG System Evaluation\n",
    "\n",
    "This notebook evaluates the RAG system using multiple metrics:\n",
    "1. Retrieval Metrics\n",
    "   - Retrieval Precision\n",
    "   - Context Relevance\n",
    "2. Generation Metrics\n",
    "   - Answer Relevance\n",
    "   - Factual Consistency\n",
    "3. Overall Metrics\n",
    "   - ROUGE Scores\n",
    "   - BERTScore\n",
    "   - Response Time\n",
    "\n",
    "All metrics are tracked using MLflow for experiment monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_relevancy,\n",
    "    faithfulness,\n",
    "    answer_relevancy\n",
    ")\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_qa_samples() -> List[Dict]:\n",
    "    \"\"\"Load sample QA pairs for evaluation\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"question\": \"What are the main topics discussed in the video?\",\n",
    "            \"expected_answer\": \"The video discusses...\",  # Replace with actual expected answer\n",
    "            \"context\": \"Full transcript section...\"  # Replace with actual context\n",
    "        },\n",
    "        # Add more QA pairs as needed\n",
    "    ]\n",
    "\n",
    "def calculate_rouge_scores(prediction: str, reference: str) -> Dict:\n",
    "    \"\"\"Calculate ROUGE scores\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(prediction, reference)\n",
    "    return {\n",
    "        'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "        'rouge2_f1': scores['rouge2'].fmeasure,\n",
    "        'rougeL_f1': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "def calculate_bert_scores(prediction: str, reference: str) -> Dict:\n",
    "    \"\"\"Calculate BERTScore\"\"\"\n",
    "    P, R, F1 = bert_score([prediction], [reference], lang='en')\n",
    "    return {\n",
    "        'bert_precision': P.item(),\n",
    "        'bert_recall': R.item(),\n",
    "        'bert_f1': F1.item()\n",
    "    }\n",
    "\n",
    "def evaluate_retrieval(retrieved_contexts: List[str], relevant_context: str) -> Dict:\n",
    "    \"\"\"Evaluate retrieval performance\"\"\"\n",
    "    # Use RAGAS metrics for context evaluation\n",
    "    context_scores = evaluate(\n",
    "        retrieved_contexts,\n",
    "        [relevant_context],\n",
    "        metrics=[context_relevancy]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'context_relevancy': context_scores['context_relevancy']\n",
    "    }\n",
    "\n",
    "def evaluate_generation(prediction: str, reference: str, context: str) -> Dict:\n",
    "    \"\"\"Evaluate answer generation\"\"\"\n",
    "    # Use RAGAS metrics for answer evaluation\n",
    "    generation_scores = evaluate(\n",
    "        [prediction],\n",
    "        [reference],\n",
    "        [context],\n",
    "        metrics=[faithfulness, answer_relevancy]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'faithfulness': generation_scores['faithfulness'],\n",
    "        'answer_relevancy': generation_scores['answer_relevancy']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the RAG system from the previous notebook\n",
    "from youtube_rag_mlflow import qa_chain, vectorstore\n",
    "\n",
    "def evaluate_rag_system(qa_samples: List[Dict]):\n",
    "    \"\"\"Evaluate RAG system performance\"\"\"\n",
    "    with mlflow.start_run(run_name=f\"rag_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
    "        all_metrics = []\n",
    "        \n",
    "        for i, sample in enumerate(qa_samples):\n",
    "            # Get RAG system response\n",
    "            start_time = time.time()\n",
    "            result = qa_chain({\"query\": sample['question']})\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            prediction = result['result']\n",
    "            retrieved_contexts = [doc.page_content for doc in result['source_documents']]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rouge_scores = calculate_rouge_scores(prediction, sample['expected_answer'])\n",
    "            bert_scores = calculate_bert_scores(prediction, sample['expected_answer'])\n",
    "            retrieval_scores = evaluate_retrieval(retrieved_contexts, sample['context'])\n",
    "            generation_scores = evaluate_generation(\n",
    "                prediction,\n",
    "                sample['expected_answer'],\n",
    "                sample['context']\n",
    "            )\n",
    "            \n",
    "            # Combine all metrics\n",
    "            metrics = {\n",
    "                'sample_id': i,\n",
    "                'response_time': response_time,\n",
    "                **rouge_scores,\n",
    "                **bert_scores,\n",
    "                **retrieval_scores,\n",
    "                **generation_scores\n",
    "            }\n",
    "            \n",
    "            # Log metrics to MLflow\n",
    "            for metric_name, value in metrics.items():\n",
    "                if metric_name != 'sample_id':\n",
    "                    mlflow.log_metric(f\"{metric_name}_{i}\", value)\n",
    "            \n",
    "            all_metrics.append(metrics)\n",
    "        \n",
    "        # Calculate and log average metrics\n",
    "        metrics_df = pd.DataFrame(all_metrics)\n",
    "        avg_metrics = metrics_df.mean(numeric_only=True)\n",
    "        \n",
    "        for metric_name, value in avg_metrics.items():\n",
    "            if metric_name != 'sample_id':\n",
    "                mlflow.log_metric(f\"avg_{metric_name}\", value)\n",
    "        \n",
    "        # Save detailed results\n",
    "        metrics_df.to_csv('evaluation_results.csv', index=False)\n",
    "        mlflow.log_artifact('evaluation_results.csv')\n",
    "        \n",
    "        return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run evaluation\n",
    "qa_samples = load_qa_samples()\n",
    "results_df = evaluate_rag_system(qa_samples)\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\nEvaluation Results Summary:\")\n",
    "print(\"============================\")\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "\n",
    "print(\"\\nMetric Distributions:\")\n",
    "print(results_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot distribution of key metrics\n",
    "metrics_to_plot = [\n",
    "    'rouge1_f1', 'bert_f1', 'context_relevancy',\n",
    "    'faithfulness', 'answer_relevancy', 'response_time'\n",
    "]\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(results_df[metric], kde=True)\n",
    "    plt.title(f'Distribution of {metric}')\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_distributions.png')\n",
    "mlflow.log_artifact('metric_distributions.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
