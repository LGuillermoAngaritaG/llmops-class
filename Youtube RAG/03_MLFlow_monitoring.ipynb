{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube RAG System with MLflow and Monitoring\n",
    "\n",
    "This notebook sets up the RAG system as an MLflow model with comprehensive monitoring:\n",
    "1. Model Registry: Version control and deployment management\n",
    "2. Performance Monitoring: Response times, resource usage\n",
    "3. Quality Monitoring: Answer relevance, context quality\n",
    "4. Drift Detection: Input and output distribution changes\n",
    "5. Dashboard: Real-time visualization of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from prometheus_client import start_http_server, Counter, Histogram, Gauge\n",
    "from evidently.metrics import DataDriftTable, DataQualityPreset\n",
    "from evidently.report import Report\n",
    "from evidently.test_suite import TestSuite\n",
    "import streamlit as st\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize monitoring metrics\n",
    "class RAGMonitoring:\n",
    "    def __init__(self):\n",
    "        # Prometheus metrics\n",
    "        self.request_count = Counter('rag_requests_total', 'Total RAG requests')\n",
    "        self.error_count = Counter('rag_errors_total', 'Total RAG errors')\n",
    "        self.response_time = Histogram('rag_response_time_seconds', 'Response time in seconds')\n",
    "        self.context_length = Histogram('rag_context_length', 'Retrieved context length')\n",
    "        self.answer_length = Histogram('rag_answer_length', 'Generated answer length')\n",
    "        self.system_load = Gauge('rag_system_load', 'System load average')\n",
    "        \n",
    "        # Historical data for drift detection\n",
    "        self.historical_data = []\n",
    "        \n",
    "    def log_request(self, query: str, context: List[str], answer: str, response_time: float):\n",
    "        \"\"\"Log request metrics\"\"\"\n",
    "        self.request_count.inc()\n",
    "        self.response_time.observe(response_time)\n",
    "        self.context_length.observe(sum(len(c) for c in context))\n",
    "        self.answer_length.observe(len(answer))\n",
    "        \n",
    "        # Store data for drift detection\n",
    "        self.historical_data.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'query_length': len(query),\n",
    "            'context_length': sum(len(c) for c in context),\n",
    "            'answer_length': len(answer),\n",
    "            'response_time': response_time\n",
    "        })\n",
    "    \n",
    "    def check_drift(self):\n",
    "        \"\"\"Check for data drift\"\"\"\n",
    "        if len(self.historical_data) < 10:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(self.historical_data)\n",
    "        report = Report(metrics=[\n",
    "            DataDriftTable(),\n",
    "            DataQualityPreset()\n",
    "        ])\n",
    "        report.run(reference_data=df.iloc[:len(df)//2], current_data=df.iloc[len(df)//2:])\n",
    "        return report\n",
    "\n",
    "# Initialize monitoring\n",
    "monitoring = RAGMonitoring()\n",
    "start_http_server(8000)  # Start Prometheus metrics server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class YouTubeRAG(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "        self.monitoring = monitoring\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load model context\"\"\"\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "        # Load vector store\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=self.config['chroma_dir'],\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # Create QA chain\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=ChatOpenAI(temperature=self.config['temperature']),\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": self.config['retriever_k']}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"Make prediction with monitoring\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Get answer from RAG system\n",
    "            result = self.qa_chain({\"query\": model_input['question']})\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Log metrics\n",
    "            self.monitoring.log_request(\n",
    "                query=model_input['question'],\n",
    "                context=[doc.page_content for doc in result['source_documents']],\n",
    "                answer=result['result'],\n",
    "                response_time=response_time\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'answer': result['result'],\n",
    "                'sources': [doc.metadata for doc in result['source_documents']],\n",
    "                'response_time': response_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.monitoring.error_count.inc()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    'chroma_dir': './chroma_db',\n",
    "    'temperature': 0,\n",
    "    'retriever_k': 3,\n",
    "    'chunk_size': 1000,\n",
    "    'chunk_overlap': 200\n",
    "}\n",
    "\n",
    "# Initialize and log model\n",
    "with mlflow.start_run(run_name=f\"rag_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_params(config)\n",
    "    \n",
    "    # Create and log model\n",
    "    rag_model = YouTubeRAG(config)\n",
    "    \n",
    "    # Log model to MLflow\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"rag_model\",\n",
    "        python_model=rag_model,\n",
    "        registered_model_name=\"youtube_rag\"\n",
    "    )\n",
    "    \n",
    "    # Save run ID for later reference\n",
    "    run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_monitoring_dashboard():\n",
    "    \"\"\"Create Streamlit dashboard for monitoring\"\"\"\n",
    "    st.title(\"YouTube RAG System Monitoring\")\n",
    "    \n",
    "    # System metrics\n",
    "    st.header(\"System Metrics\")\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    with col1:\n",
    "        st.metric(\"Total Requests\", monitoring.request_count._value.get())\n",
    "    with col2:\n",
    "        st.metric(\"Total Errors\", monitoring.error_count._value.get())\n",
    "    with col3:\n",
    "        st.metric(\"Avg Response Time\", f\"{np.mean([h['response_time'] for h in monitoring.historical_data]):.2f}s\")\n",
    "    \n",
    "    # Performance trends\n",
    "    st.header(\"Performance Trends\")\n",
    "    if monitoring.historical_data:\n",
    "        df = pd.DataFrame(monitoring.historical_data)\n",
    "        st.line_chart(df.set_index('timestamp')['response_time'])\n",
    "    \n",
    "    # Drift detection\n",
    "    st.header(\"Drift Detection\")\n",
    "    drift_report = monitoring.check_drift()\n",
    "    if drift_report:\n",
    "        st.write(drift_report.json())\n",
    "    else:\n",
    "        st.write(\"Not enough data for drift detection\")\n",
    "\n",
    "# Save dashboard script\n",
    "with open('monitoring_dashboard.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "import streamlit as st\n",
    "from youtube_rag_monitoring import create_monitoring_dashboard\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_monitoring_dashboard()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example usage\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/rag_model\")\n",
    "\n",
    "# Test questions\n",
    "questions = [\n",
    "    \"What are the main topics discussed in these videos?\",\n",
    "    \"Can you summarize the key points?\",\n",
    "    \"What are the main conclusions?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = loaded_model.predict({'question': question})\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Response time: {result['response_time']:.2f}s\")\n",
    "    print(\"Sources:\")\n",
    "    for source in result['sources']:\n",
    "        print(f\"- {source['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Setup Instructions\n",
    "\n",
    "1. Start the monitoring dashboard:\n",
    "```bash\n",
    "streamlit run monitoring_dashboard.py\n",
    "```\n",
    "\n",
    "2. View Prometheus metrics:\n",
    "```bash\n",
    "curl localhost:8000/metrics\n",
    "```\n",
    "\n",
    "3. Access MLflow UI:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "The dashboard provides real-time monitoring of:\n",
    "- System metrics (requests, errors, response times)\n",
    "- Performance trends\n",
    "- Data drift detection\n",
    "- Model versioning and deployment status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
